Рейтинг https://docs.google.com/spreadsheets/d/1NSdcHzMy_KoFx6hSZUuXUTwW5X6xsFY-POW2wQ4jjjs/edit 
Решения домашек https://cloud.mail.ru/public/312c/kfJQFGKCK

Week 1
------------------------------------------------------------
Week 2

Решение 1 домашки – https://goo.gl/hYCjxa
2 статья (обновление) – https://habrahabr.ru/company/ods/blog/323210/
2 домашка – https://goo.gl/QQGK3p Дедлайн: 19 сентября 23:59 UTC +3. Автор – @lucidyan

Рейтинг https://docs.google.com/spreadsheets/d/1NSdcHzMy_KoFx6hSZUuXUTwW5X6xsFY-POW2wQ4jjjs/edit 

Видео лекция 2 https://www.youtube.com/watch?v=vm63p8Od0bM

mini-FAQ по второй домашке

Не первый раз уже проскакивает вопрос про чистку данных - отвечаю.
Задание формировалось для лучшего знакомства с различными типами визуализаций и способов их использования, подразумевалось, что в процессе вы и получите лучшее понимание, где что зашумлено и как друг с другом связано. 

В заданиях ничего не написано про дополнительные манипуляции с данными: все ответы должны получаться БЕЗ очистки данных!

Но никто не запрещает вам поиграться с очищенными данными - немаловероятно, что вы найдете крутые закономерности, которые ушли незамеченными от других. Постите их в чат, всем будет интересно! :coolstorybob:

Задание 1
Почему корреляция с категориальными переменными может иметь смысл
https://opendatascience.slack.com/archives/C39147V60/p1505399479000343

Задание 2
Про смысл ViolinPlot
https://opendatascience.slack.com/archives/C39147V60/p1505295700000235

Задание 3
Про природу данных и шумы
https://opendatascience.slack.com/archives/C39147V60/p1505474469000082

Задание 4
Про критерий подсчета кластеров
https://opendatascience.slack.com/archives/C39147V60/p1505313702000742

Задание 5
Можно использовать и BarPlot и CountPlot, но второй для этих целей проще
https://opendatascience.slack.com/files/U6URRB952/F73GBFVU3/__________________________2017-09-15____11.08.20.png

-------------------------------------------------------------------------------------------------------------------------------
И специальная ссылка, для тех, кто не умеет в закрепленные сообщения. Очень важно!  :putin:
https://opendatascience.slack.com/archives/C39147V60/p1504603983000144

------------------------------------------------------------------------------------

Пятничное видео – беседа с А.Г. Дьяконовым. Про Kaggle, карьеру, образование, роль советских ученых в становлении ML и про то, зачем нужен наш открытый курс по машинному обучению, и куда девать столько датасаентистов https://www.youtube.com/watch?v=qV3yjIyj7Dc

Блог Александра Дьяконова: “анализ малых данных” https://alexanderdyakonov.wordpress.com/
-----------------------------------------------------------------------------------------

Отличные материалы как раз по визуализации от автора известной книжки про пандас https://m.youtube.com/watch?v=FytuB8nFHPQ

-----------------------------------------------------------------------------------------
Week 3

Решение 2 домашки – https://goo.gl/cuoH6F (автор – @lucidyan)
З статья – https://habrahabr.ru/company/ods/blog/322534/
Домашка №3 – https://goo.gl/787DsT (автор – @yorko)
Дедлайн: 26  сентября 23:59 UTC+3

Видео лекция 3 https://www.youtube.com/watch?v=p9Hny3Cs6rk


Всем привет, решил поделиться ссылкой на вводный туториал по "Деревьям принятия решений и классификаторам". Неплохая разминка перед 3-й лекцией будет :slightly_smiling_face:
https://www.youtube.com/watch?v=cKxRvEZd3Mw

Советую всем, кто чувствует необходимость, прочитать еще и конспект лекции Евгения Соколова по деревьям решений https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture07-trees.pdf

*FAQ по 3 домашке*

*Вопрос 1*. Первые 2 вопроса – на построение дерева-регрессора от руки, тут не надо использовать DecisionTreeRegressor и уж тем паче DecisionTreeClassifier
*Вопрос 2*. Здесь считаются и вертикальные отрезки тоже.
*Вопрос 3*. Тут вроде нет проблем 
*Вопрос 4*. Определения четкого пика не дано, но если все сделать правильно, то понятно. Имеется в виду максимум качества (доли верных ответов) как функции от max_depth для значения max_depth  > 2 и < 10. Про повышение на 1%: надо посмотреть на выражение (acc2 - acc1) / acc1 * 100%, где acc1 и acc2 –  доли верных ответов на отложенной выборке до и после настройки max_depth соотв-но.
*Вопрос 5*. Тут надо построить дерево только по 12-ти описанным бинарным признакам, исходные признаки не берем

-------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------

Добавил опциональную домашку для любителей алгоритмов и программирования – реализовать алгоритм построения дерева решений. https://goo.gl/m7XaGQ Это слишком  жестко для начинающих (каковых тут большинство), тут и ООП, и вообще думать больше надо, к тому же сложно устроить проверку, поскольку задание слегка более креативное, и многое может “пойти не так“, поэтому задание не за баллы – чисто для тех, кто хочет покопаться, оно, несомненно, полезно, если хочется досканально разобраться в деревьях. Решение выложу тоже во вторник

-------------------------------------------------------------------------------------------
я выпилил из репозитория решения домашек, теперь они тут https://cloud.mail.ru/public/312c/kfJQFGKCK

------------------------------------------------------------------------------------------
Week 4

Ссылка на трансляцию 4-ой лекции https://www.youtube.com/watch?v=oTXGQ-_oqvI

*Решение 3 домашки* – в облаке https://cloud.mail.ru/public/312c/kfJQFGKCK копируйте, смотрите у себя локально (есть причины на то, чтоб не держать их в репозитории), дальнейшие решения сюда же будут выкладываться. Реализация дерева решений тоже там.

Ссылка для *регистрации на 5-ю лекцию* https://goo.gl/vbHuvB

*Статья №4* (модиф) – https://habrahabr.ru/company/ods/blog/323890/ Ее нужно подробно и вдумчиво прочитать. И чтобы домашку сделать, и вообще для жизни). Вначале там идет регрессия – это будет разбираться на 6-ой лекции (контент 6-ой статьи, соотв-но, на ней не будет разбираться). 

*Домашка №4* – автор @mephistopheies https://goo.gl/RAxGLA Задание сложное, на него 2 недели и 20 баллов, дедлайн: *10 октября*, 23:59 UTC+3. Не откладывайте до последнего, потом к тому же 10 октября добавится домашка №5, попроще, но тоже на важнейшую тему. Перед этой домашкой лучше побить первые два бенчмарка нашего контеста, на 4 лекции это будет частично делаться.

Активно читайте канал и задавайте вопросы с *тегами* (пытаемся уменьшить энтропию в канале):
*#w4_t1, … #w4_t10* – по десяти пунтам 4-ой домашки соотв-но (“w” for “week”, “t” for “task”)
*#proj_indiv* – по индивидуальным проектам (завтра объявим критерии проверки, которые как раз и служат гайдлайном в выполнении проекта)
*#proj_alice* – по проекту Элис и связанному соревнованию 
*#tutorial_contest* – по тьюториалам
*#vis_contest* – новый конкурс по визуализации

*Второй конкурс по визуализации*
Все так же, как и в первый раз, но с вопросами StackOverflow (датасет из 4 домашки stackoverflow_sample_125k.tsv), как раз с текстами повозиться. Кидаете сюда (*обязательно* с тегом *#vis_contest*), чья картинка набирает больше всего :heavy_plus_sign: – тому респект и

1 место (по плюсам) – 7 баллов в рейтинг :muscle:
2 место – 5 баллов :notbad:
3 место  – 4 балла :regression:

Если картинка просто понравилась, но баллы бы вы за нее не дали – просто поставьте :+1: (это касается также лулзов и смехуечков)

Подсчет баллов будет *3 октября* в 00:00.

yorko [10:35 AM] 
Квант демократии: разносим дедлайны по 4-ой домашке (уже точно 10 октября) и 5-ой (обнародуется 4-го октября)? :heavy_plus_sign: или :heavy_minus_sign: Если разносим, то по 5-ой домашке это будет 11-ое октября

-----------------------------------------------------------------------------------------
Табличка с математическими символами для тех, кто уже посмотрел четвертую домашку и перестал плакать — http://www.rapidtables.com/math/symbols/Basic_Math_Symbols.htm

Именно эту табличка рекомендуется в курсе "Теория вероятностей", который стартовал на stepik.org   https://stepik.org/course/3089/syllabus 


------------------------------------------------------------------------------------------
домучил наконец docker-образ, теперь там есть и dot с graphviz. В обновленном нубуке check_docker.ipynb https://goo.gl/Y7axGT можно попробовать – деревцо нарисовал как раз запустив юпитер из докера. 

python run_docker_jupyter.py, далее localhost:4545

------------------------------------------------------------------------------------------
Статья несколько дней назад вышла, но все же поделюсь: “Достижения в глубоком обучении за последний год” https://habrahabr.ru/company/mailru/blog/338248/ Если вы мало что поняли по части нейронных сетей – это :norma: Если кто-то утверждает, что нейронные сети – современная гомеопатия, его можно ткнуть в эту статью. Ну а для нас это мотивация догонять, чтоб все понимать и в подобных статьях, и в оригиналах. То есть для начала пройти cs231n.
Коль уж отвлекся, порекомендую этот канал для тех, кому тут “скучно” https://www.youtube.com/playlist?list=PLtPJ9lKvJ4oiz9aaL_xcZd-x0qd8G0VN_ Товарищ вещает про нейронки очень кратко, по сути, без математики, а так чтоб сразу взять и что-то запилить. Такой подход хорошо дополняется чтением Deep Learning book и прохождением курсов. После этого можно и :kaggle: покорять


------------------------------------------------------------------------------------------

Коллеги, собрала небольшой список мест, откуда можно брать данные для своего проекта.
0). 
Канал #datasets
1).
https://archive.ics.uci.edu/ml/datasets.html
2).
https://www.dataquest.io/blog/free-datasets-for-projects/
3).
https://github.com/caesar0301/awesome-public-datasets
4).
https://elitedatascience.com/datasets
5).
https://www.kaggle.com/datasets
6). https://docs.google.com/spreadsheets/d/1ZSLP1McnXv0FtOd9t7dMp3AfaiusvaGwWV0F9g2pbho/edit#gid=0
7).
https://goo.gl/1Hu3At

8).
https://opendata.cityofnewyork.us/
9).
https://datarepository.wolframcloud.com/
10).
https://www.datapure.co/open-data-sets
11).
https://www.data.gov/


Ещё данные можно самостоятельно собирать из статистических сервисов разных стран (как Росстат, например).
Если у вас есть ещё ссылки на места, где водятся данные, прошу, напишите сюда.

https://www.kaggle.com/datasets

вот большая относительно подборка датасетов https://docs.google.com/spreadsheets/d/1ZSLP1McnXv0FtOd9t7dMp3AfaiusvaGwWV0F9g2pbho/edit#gid=0

-------------------------------------------------------------------------------------

*Индивидуальные проекты*

Добавил описание проекта – вместе с описанием сразу всех наших активностей https://goo.gl/cJbw7V. Еще будет уточняться, про критерии оценок еще отдельно напишем. Вкратце: это будет Peer-review, примерно как на курсере.

Шаблон проекта https://goo.gl/f71rH5 Сегодня на лекции обсудим, пример потом тоже будет. 

Как определилсь, записаться с темой проекта надо сюда https://goo.gl/itRBtw (у всех будут разные). Если идей нет – скоро опишем, какие данные лучше взять. Или тогда берите проект Элис, где все по шагам. 

Времени тоже примерно до 8 ноября. ближе к делу сделаем так, чтоб все дедлайны на один день не выпали.

С вопросами – в этот тред, в основном к @lucidyan. Если что, я тоже подскажу



#proj_indiv Добавил 4 примера инд. проектов https://github.com/Yorko/mlcourse_open/tree/master/jupyter_notebooks/projects_individual Чисто as is (косяки в них правиться не будут) – в качестве примера. У нас план проекта немного поменяется, сегодня вечером все уточню. Но это хорошие примеры того, что в целом ожидается.
banks, telecom_churn – попроще, остальные два – поинтересней


------------------------------------------------------------------------------------------

Книги и материалы https://ru.stackoverflow.com/questions/678970/%D0%9A%D0%BD%D0%B8%D0%B3%D0%B8-%D0%B8-%D1%83%D1%87%D0%B5%D0%B1%D0%BD%D1%8B%D0%B5-%D1%80%D0%B5%D1%81%D1%83%D1%80%D1%81%D1%8B-%D0%BF%D0%BE-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%BC%D1%83-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8E/683632#683632





